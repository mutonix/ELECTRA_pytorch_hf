{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import datasets\n",
    "from transformers import ElectraConfig, ElectraTokenizerFast, ElectraForMaskedLM, ElectraForPreTraining\n",
    "from transformers import TrainingArguments, AdamW\n",
    "\n",
    "from data_utils import MyConfig, ElectraDataProcessor, ElectraDataCollator\n",
    "from train_utils import ElectraModel, ElectraLoss, ElectraTrainer, ElectraWandbCallback\n",
    "\n",
    "os.environ['WANDB_PROJECT'] = 'electra_pretrain'\n",
    "os.environ['WANDB_WATCH'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = MyConfig({\n",
    "    'base_run_name': 'origin', # run_name = {base_run_name}_{seed}\n",
    "    'seed': 11081, # 11081 36 1188 76 1 4 4649 7 # None/False to randomly choose seed from [0,999999]\n",
    "\n",
    "    'adam_bias_correction': False,\n",
    "    'sampling_method': 'fp32_gumbel',\n",
    "    'from_pretrained': True,\n",
    "\n",
    "    'size': 'small',\n",
    "    'datas': ['my_text'],\n",
    "\n",
    "    'logger': 'wandb',\n",
    "    'num_proc': 1,\n",
    "    'num_workers': 0,\n",
    "    'n_gpus': 8,\n",
    "})\n",
    "\n",
    "# Check and Default\n",
    "assert c.sampling_method in ['fp32_gumbel', 'fp16_gumbel', 'multinomial']\n",
    "for data in c.datas: assert data in ['wikipedia', 'bookcorpus', 'openwebtext', 'my_text']\n",
    "assert c.logger in ['wandb', 'neptune', None, False]\n",
    "if not c.base_run_name: c.base_run_name = str(datetime.now(timezone(timedelta(hours=+8))))[6:-13].replace(' ','').replace(':','').replace('-','')\n",
    "if not c.seed: c.seed = random.randint(0, 999999)\n",
    "c.run_name = f'{c.base_run_name}_{c.seed}'\n",
    "\n",
    "# Setting of different sizes\n",
    "i = ['small', 'base', 'large'].index(c.size)\n",
    "c.mask_prob = [0.15, 0.15, 0.25][i]\n",
    "c.lr = [5e-4, 2e-4, 2e-4][i]\n",
    "c.bs = [128, 256, 2048][i]\n",
    "c.steps = [10**6, 766*1000, 400*1000][i]\n",
    "c.max_length = [128, 512, 512][i]\n",
    "generator_size_divisor = [4, 3, 4][i]\n",
    "disc_config = ElectraConfig.from_pretrained(f'google/electra-{c.size}-discriminator')\n",
    "gen_config = ElectraConfig.from_pretrained(f'google/electra-{c.size}-generator')\n",
    "\n",
    "# note that public electra-small model is actually small++ and don't scale down generator size \n",
    "gen_config.hidden_size = int(disc_config.hidden_size / generator_size_divisor)\n",
    "gen_config.num_attention_heads = disc_config.num_attention_heads // generator_size_divisor\n",
    "gen_config.intermediate_size = disc_config.intermediate_size // generator_size_divisor\n",
    "hf_tokenizer = ElectraTokenizerFast.from_pretrained(f\"google/electra-{c.size}-generator\")\n",
    "\n",
    "# Path to data\n",
    "Path('./datasets').mkdir(exist_ok=True)\n",
    "Path('./checkpoints/pretrain').mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load/download my text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-996098ba8a5ac029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to ./datasets\\text\\default-996098ba8a5ac029\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 501.17it/s]\n",
      "                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to ./datasets\\text\\default-996098ba8a5ac029\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 330.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load/create data from my text for ELECTRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.51ba/s]\n"
     ]
    }
   ],
   "source": [
    "dsets = []\n",
    "ElectraProcessor = partial(ElectraDataProcessor, hf_tokenizer=hf_tokenizer, max_length=c.max_length, apply_filter=False)\n",
    "\n",
    "if 'wikipedia' in c.datas:\n",
    "    print('load/download wiki dataset')\n",
    "    wiki = datasets.load_dataset('wikipedia', '20200501.en', cache_dir='./datasets')['train']\n",
    "    print('load/create data from wiki dataset for ELECTRA')\n",
    "    e_wiki = ElectraProcessor(wiki).map(cache_file_name=f\"electra_wiki_{c.max_length}.arrow\", num_proc=c.num_proc)\n",
    "    dsets.append(e_wiki)\n",
    "\n",
    "# OpenWebText\n",
    "if 'openwebtext' in c.datas:\n",
    "    print('load/download OpenWebText Corpus')\n",
    "    owt = datasets.load_dataset('openwebtext', cache_dir='./datasets')['train']\n",
    "    print('load/create data from OpenWebText Corpus for ELECTRA')\n",
    "    e_owt = ElectraProcessor(owt).map(cache_file_name=f\"electra_owt_{c.max_length}.arrow\", num_proc=c.num_proc)\n",
    "    dsets.append(e_owt)\n",
    "\n",
    "if 'my_text' in c.datas:\n",
    "    print('load/download my text')\n",
    "    mt = datasets.load_dataset(\"text\", data_files={\"train\": \"urlsf_subset00-944_data.txt\"}, cache_dir='./datasets')['train']\n",
    "    print('load/create data from my text for ELECTRA')\n",
    "    e_mytext = ElectraProcessor(mt).map(cache_file_name=f\"electra_mt_{c.max_length}.arrow\", num_proc=c.num_proc)\n",
    "    dsets.append(e_mytext)\n",
    "\n",
    "assert len(dsets) == len(c.datas)\n",
    "\n",
    "electra_dset = datasets.concatenate_datasets(dsets)\n",
    "electra_dset.set_format(type='torch', columns=['input_ids', 'sentA_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pre-Training\"\"\"\n",
    "torch.backends.cudnn.benchmark = True\n",
    "random.seed(c.seed)\n",
    "np.random.seed(c.seed)\n",
    "torch.manual_seed(c.seed)\n",
    "\n",
    "if c.size in ['base', 'large'] and c.from_pretrained:\n",
    "    generator = ElectraForMaskedLM.from_pretrained(f\"google/electra-{c.size}-generator\")\n",
    "    discriminator = ElectraForPreTraining.from_pretrained(f\"google/electra-{c.size}-discriminator\")\n",
    "else:\n",
    "    generator = ElectraForMaskedLM(gen_config)\n",
    "    discriminator = ElectraForPreTraining(disc_config)\n",
    "    discriminator.electra.embeddings = generator.electra.embeddings\n",
    "    generator.generator_lm_head.weight = generator.electra.embeddings.word_embeddings.weight\n",
    "\n",
    "electra_model = ElectraModel(generator, discriminator, hf_tokenizer, sampling_method=c.sampling_method)\n",
    "electra_loss_func = ElectraLoss(loss_weights=(1.0, 50.0))\n",
    "\n",
    "electra_data_collator = ElectraDataCollator(hf_tokenizer, c.max_length)\n",
    "AdamW_no_bias  = AdamW(electra_model.parameters(), lr=c.lr, betas=(0.9, 0.99), eps=1e-5, weight_decay=0.01, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Initialize args')\n",
    "training_args = TrainingArguments(\n",
    "    run_name=f'{c.base_run_name}-{c.size}',\n",
    "    output_dir=f'./pretrain/checkpoints/{c.base_run_name}-{c.size}',          # output directory\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=1,\n",
    "    save_steps=10000,     # Number of updates steps before two checkpoint saves. default: 500\n",
    "    save_total_limit=20, # If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir.\n",
    "    dataloader_num_workers=c.num_workers,\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=c.bs // c.n_gpus,  # batch size per device during training\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_steps=10000,\n",
    "    max_steps=1000000,   # 100k\n",
    "    seed = c.seed,\n",
    "    fp16=True,\n",
    "    # report_to=['wandb'],\n",
    "    # deepspeed='ds_config.json',\n",
    ")\n",
    "\n",
    "print('Initialize trainer')\n",
    "trainer = ElectraTrainer(\n",
    "    model=electra_model,                 # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=electra_dset,          # training dataset\n",
    "    data_collator=electra_data_collator,\n",
    "    optimizers=(AdamW_no_bias, None),\n",
    "    callbacks=[ElectraWandbCallback],\n",
    "    loss_func=electra_loss_func,\n",
    ")\n",
    "\n",
    "print('Start training at ', datetime.now())\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d284eb6acfe5deb46986cab86e3b34ab256b51e881523707750a6c60702d3cbf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('yolo_v5': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
